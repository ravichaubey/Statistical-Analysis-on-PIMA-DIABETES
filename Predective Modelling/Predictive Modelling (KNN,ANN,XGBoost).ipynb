{"cells":[{"metadata":{},"cell_type":"markdown","source":"I have made 4 notebook on this dataset to show Statistics and Machine Learning. You can read all of them here ==>\n\n1. [Univariate Statistical Analysis](https://www.kaggle.com/ravichaubey1506/univariate-statistical-analysis-on-diabetes)\n2. [Multivariate Staistical Analysis](https://www.kaggle.com/ravichaubey1506/multivariate-statistical-analysis-on-diabetes)\n3. [Inferencial Statistics](https://www.kaggle.com/ravichaubey1506/inferential-statistics-on-diabetes)\n4. [Predective Modelling on Diabtes](https://www.kaggle.com/ravichaubey1506/predictive-modelling-knn-ann-xgboost/)"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":false},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline\nsns.set_style('darkgrid')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df = pd.read_csv('../input/pima-indians-diabetes-database/diabetes.csv')\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Introduction\n\nLet us  do some EDA to see behaviour of data which will help in Preprocessing."},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Shape of Data is ==> \",df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Summary\n\nData is related to healthcare Industry having 768 observations with 9 variable. Target variable is Outcome. It looks like there is no missing value, and boolean, float , integers are different datatypes available. Well descriptive analysis shows that variable Glucose, BoodPressure,SckinThickness, Insulin and BMI have minimum value 0 which does not make any sense, these values are either missing or outliers, But i am not going to alter them so that i can see actual statistics of Data. I can see in Pregnancies column, minimum is 0 (May be this is sign for no pregnancy) which is considerable, But maximum month of pregnancy is 17 which does not make any sense. Variance among different predictor variable is varying at large scale , Scaling data will be helpful for Predective modelling."},{"metadata":{},"cell_type":"markdown","source":"# Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"**Let us check column names first and manipulate if any change needed.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in df.columns:\n    print(i)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**I will change DiabtesPedigreeFunction to DPF for conviniene**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.rename({'DiabetesPedigreeFunction':'DPF'},inplace = True,axis =1)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Let us check datatypes of variables**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Everything is perfect.**"},{"metadata":{},"cell_type":"markdown","source":"# Preprocessing\n\nYou might get confuse what is difference between Data Cleaning and Data Preprocessing?\n\nWell Data Preprocessing is beyong Data Cleaning is used to Make data tidy. Data Preprocessing is used to make data in way such that we can fit model to it."},{"metadata":{},"cell_type":"markdown","source":"## Missing Values & Outliers\n\nLet us look to missing values and handle them. \nFirst I create function to handle outliers.Standard deviation based detection."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"def std_based(col_name,df):\n    mean = df[col_name].mean()\n    std = df[col_name].std()\n    cut_off = std * 3\n    lower, upper = mean - cut_off, mean + cut_off\n    new_df = df[(df[col_name] < upper) & (df[col_name] > lower)]\n    return new_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color='red'> It looks like there is no missing values. But in descriptive statistics we have seen that some variables have minimum = 0 and pregnancy variable has maximum = 17 which is not making sense. So let us explore these variables and treat them accordingly.</font>**\n\n**Please note that in my last 3 notebooks we have seen that variables are following normal distribution , So with Statistical Evidence we can fill values using Mean, Median and Mode.**\n\nOther Notebook in this series are ==>\n\n1. [Univariate Statistical Analysis](https://www.kaggle.com/ravichaubey1506/univariate-statistical-analysis-on-diabetes)\n2. [Multivariate Staistical Analysis](https://www.kaggle.com/ravichaubey1506/multivariate-statistical-analysis-on-diabetes)\n3. [Inferencial Statistics](https://www.kaggle.com/ravichaubey1506/inferential-statistics-on-diabetes)"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Pregnancies'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that minimum is 0 which may be considered as no Pregnancy, But maximum is 17 which is not making sense. Let us see distribution and also boxplot for outliers"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Pregnancies'],ax=axes[0],color='m')\naxes[0].set_title('Distribution of Pregnancy',fontdict={'fontsize':8})\naxes[0].set_xlabel('Pregnancy Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Pregnancies',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#Treating Outlier and then verifying it\n\ndf = std_based('Pregnancies',df)\n\nfig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Pregnancies'],ax=axes[0],color='red')\naxes[0].set_title('Distribution of Pregnancy',fontdict={'fontsize':8})\naxes[0].set_xlabel('Pregnancy Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Pregnancies',data=df,ax=axes[1],orient = 'v',color='yellow')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*<font color = 'blue'> Well, we are done with Pregnancy variable. Let us see next one. </font>*"},{"metadata":{"trusted":true},"cell_type":"code","source":"df['Glucose'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'> Glucose = 0, does not make any sense. </font>"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Glucose'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of Glucose',fontdict={'fontsize':8})\naxes[0].set_xlabel('Glucose Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Glucose',data=df,ax=axes[1],orient = 'v',color='m')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'> There is no outlier and also distribution is normal , So i will treat 0 with mean value.</font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Glucose = df.Glucose.replace(0,df.Glucose.mean())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Glucose'],ax=axes[0],color='r')\naxes[0].set_title('Distribution of Glucose',fontdict={'fontsize':8})\naxes[0].set_xlabel('Glucose Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Glucose',data=df,ax=axes[1],orient = 'v',color='y')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<font color = 'blue'> Well, done with Glucose also,Let us see next. </font>"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.BloodPressure.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We need to look at BP=0**"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['BloodPressure'],ax=axes[0],color='m')\naxes[0].set_title('Distribution of BloodPressure',fontdict={'fontsize':8})\naxes[0].set_xlabel('BloodPressure Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('BloodPressure',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like there are few Outliers at both higher end and lower end. But at higher end maximum BP is 122, So it is considerable. Now at lower end BP near 25 is not making sense. So i will treat missing value with medium and then i will also treat outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.BloodPressure = df.BloodPressure.replace(0,df.BloodPressure.median())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df  = std_based('BloodPressure',df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['BloodPressure'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of BloodPressure',fontdict={'fontsize':8})\naxes[0].set_xlabel('BloodPressure Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('BloodPressure',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well , Let us see next one."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SkinThickness.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us look at 0 SkinThickness."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['SkinThickness'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of SkinThickness',fontdict={'fontsize':8})\naxes[0].set_xlabel('SkinThickness Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('SkinThickness',data=df,ax=axes[1],orient = 'v',color='m')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.SkinThickness = df.SkinThickness.replace(0,df.SkinThickness.mean())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = std_based(\"SkinThickness\",df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['SkinThickness'],ax=axes[0],color='green')\naxes[0].set_title('Distribution of SkinThickness',fontdict={'fontsize':8})\naxes[0].set_xlabel('SkinThickness Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('SkinThickness',data=df,ax=axes[1],orient = 'v',color='m')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Can you see , I am using plot twice one before treating and another after treatment. Look at changes :), Let us see next variable.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Insulin.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Insulin'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of Insulin',fontdict={'fontsize':8})\naxes[0].set_xlabel('Insulin Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Insulin',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see there are many outliers. So i will fill 0 with Median of Insulin. I will also treat Outliers after removing zero.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Insulin = df.Insulin.replace(0,df.Insulin.median())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = std_based('Insulin',df)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Insulin'],ax=axes[0],color='r')\naxes[0].set_title('Distribution of Insulin',fontdict={'fontsize':8})\naxes[0].set_xlabel('Insulin Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Insulin',data=df,ax=axes[1],orient = 'v',color='m')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Please observe the scale at Y axis to see if outliers has been treated to some extent :) **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.BMI.describe()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['BMI'],ax=axes[0],color='b')\naxes[0].set_title('Distribution of BMI',fontdict={'fontsize':8})\naxes[0].set_xlabel('BMI Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('BMI',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"** Outliers are considerable, So i will replace zero with mean. **"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.BMI = df.BMI.replace(0,df.BMI.mean())\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['BMI'],ax=axes[0],color='m')\naxes[0].set_title('Distribution of BMI',fontdict={'fontsize':8})\naxes[0].set_xlabel('BMI Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('BMI',data=df,ax=axes[1],orient = 'v',color='c')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.DPF.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Well things is fine here, Let us see for Outliers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['DPF'],ax=axes[0],color='green')\naxes[0].set_title('Distribution of DPF',fontdict={'fontsize':8})\naxes[0].set_xlabel('DPF Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('DPF',data=df,ax=axes[1],orient = 'v',color='m')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Outliers are present at higher end. Let us treat them."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = std_based('DPF',df)\n\nfig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['DPF'],ax=axes[0],color='green')\naxes[0].set_title('Distribution of DPF',fontdict={'fontsize':8})\naxes[0].set_xlabel('DPF Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('DPF',data=df,ax=axes[1],orient = 'v')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Age.describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us take a look for outliers."},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"fig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Age'],ax=axes[0],color='green')\naxes[0].set_title('Distribution of Age',fontdict={'fontsize':8})\naxes[0].set_xlabel('Age Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Age',data=df,ax=axes[1],orient = 'v')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"df = std_based('Age',df)\n\nfig,axes = plt.subplots(nrows=1,ncols=2,dpi=120,figsize = (8,4))\n\nplot00=sns.distplot(df['Age'],ax=axes[0],color='green')\naxes[0].set_title('Distribution of Age',fontdict={'fontsize':8})\naxes[0].set_xlabel('Age Class',fontdict={'fontsize':7})\naxes[0].set_ylabel('Frequency/Distrubtion',fontdict={'fontsize':7})\nplt.tight_layout()\n\n\nplot01=sns.boxplot('Age',data=df,ax=axes[1],orient = 'v')\naxes[1].set_title('Five Point Summary',fontdict={'fontsize':8})\nplt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color = 'red'> Now we are done with missing value and Outliers. Let us take a look at data and then move ahead with other steps.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Everything is fine. Let us move to next step. There is no categorical variable, So we need not to worry about encoding. **"},{"metadata":{},"cell_type":"markdown","source":"## Statistical Assumption\n\nLet us check for some assumption like variance. Distribution is absolutely fine. We have already seen in other notebooks."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.var()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**<font color = 'red'> Variance is varying to a greater extent, So i will standardize.</font>** I am removing dpf because variance is very low."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.drop('DPF',axis = 1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Spliting Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.Outcome.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(df['Outcome']).set_title('Distribution of Outcome')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that Outcome is balance so we need not to **Stratify** data."},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.iloc[:,:-1].values\ny=df.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state = 0)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nx_train_std = ss.fit_transform(x_train)\nx_test_std = ss.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# KNN\n\nI will not use linear classifier, Please see my 2nd notebook to find why i am not using?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nknn = KNeighborsClassifier()\n\nparam_grid = {'n_neighbors':[5,10,15,25,30,50]}\n\ngrid_knn = GridSearchCV(knn,param_grid,scoring='accuracy',cv = 10,refit = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_knn.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_knn.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_knn.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_knn.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_knn.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"probs = grid_knn.predict_proba(x_test_std)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(dpi=100)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Decision Tree Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n\nparam_grid = {'criterion':['gini','entropy'],'max_depth':np.arange(2,10),'min_samples_leaf':[0.2,0.4,0.6,0.8,0.9,1]}\n\ngrid_dtc = GridSearchCV(dtc,param_grid,scoring='accuracy',cv = 10,refit = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_dtc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_dtc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_dtc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_dtc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_dtc.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"probs = grid_dtc.predict_proba(x_test_std)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(dpi=100)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SVC"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.svm import SVC\n\nsvc = SVC(probability=True)\n\nparam_grid = {'kernel':['rbf','linear'],'C':[0.01,0.1,1,0.001],'gamma':[0.1,0.01,0.2,0.4]}\n\ngrid_svc = GridSearchCV(svc,param_grid,scoring='accuracy',cv = 10,refit = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_svc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_svc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_svc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_svc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_svc.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"probs = grid_svc.predict_proba(x_test_std)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(dpi=100)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**We can see SVC is doing better than KNN and Decision Tree. Let us combine these models and see if we can improve accuracy.**"},{"metadata":{},"cell_type":"markdown","source":"# Voting Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\nclassifiers = [('knn',grid_knn),('tree',grid_dtc),('svc',grid_svc)]\n\nvtc = VotingClassifier(classifiers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vtc.fit(x_train_std,y_train)\nprint(\"Accuracy on Test set ==> \", vtc.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**SVC is doing good till now. Let us see if Random Forest, XGBoost and ANN can help to achieve more accuracy.**"},{"metadata":{},"cell_type":"markdown","source":"# Feature Selection\n\nLet us first use RFE to select important features."},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\n\nfor i in range(2,7):\n    rfe = RFE(estimator=RandomForestClassifier(),n_features_to_select=i, verbose=0)\n    rfe.fit(x_train_std,y_train)\n    print(f\"Accuracy with Feature {i} ==>\",metrics.accuracy_score(y_test, rfe.predict(x_test_std)))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"rfe = RFE(estimator=RandomForestClassifier(),n_features_to_select=5, verbose=0)\nrfe.fit(x_train_std,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Important Features are ==> \",list(df.columns[:7][rfe.support_]))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Sometime keeping unwanted variable increase variance in model. Let us see if we can improve accuracy by removing them.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"x=df.loc[:,list(df.columns[:7][rfe.support_])].values\ny=df.iloc[:,-1].values\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.20, random_state = 0)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\n\nx_train_std = ss.fit_transform(x_train)\nx_test_std = ss.transform(x_test)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Random Forest Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\n\nparam_grid = {'n_estimators':[200,500,1000],\n              'max_depth':[2,3,4,5],\n              'min_samples_leaf':[0.2,0.4,0.6,0.8,1],\n              'max_features':['auto','sqrt'],\n              'criterion':['gini','entropy']}\n\ngrid_rfc = RandomizedSearchCV(rfc,param_grid,n_iter=20,scoring='accuracy',cv = 10,refit = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_rfc.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_rfc.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_rfc.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_rfc.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_rfc.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"probs = grid_rfc.predict_proba(x_test_std)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(dpi=100)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# XGBoost Classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"import xgboost as xgb\n\nxgbcl = xgb.XGBClassifier()\n\nparam_grid = {'booster':['gbtree','gblinear'],\n             'colsample_bytree':[0.4,0.6,0.8,1],\n             'learning_rate':[0.01,0.1,0.2,0.4],\n             'max_depth':[2,3,4,6],\n             'n_estimators':[200,300,400,500],\n              'subsample':[0.4,0.6,0.8,1]}\n\ngrid_xgb = RandomizedSearchCV(xgbcl,param_grid,n_iter=30,scoring='accuracy',cv = 10,refit = True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"grid_xgb.fit(x_train_std,y_train)\nprint(\"Best Score ==> \", grid_xgb.best_score_)\nprint(\"Tuned Paramerers ==> \",grid_xgb.best_params_)\nprint(\"Accuracy on Train set ==> \", grid_xgb.score(x_train_std,y_train))\nprint(\"Accuracy on Test set ==> \", grid_xgb.score(x_test_std,y_test))","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"probs = grid_xgb.predict_proba(x_test_std)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(dpi=100)\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ANN"},{"metadata":{"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential()\n\nclassifier.add(Dense(units= 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 5))\nclassifier.add(Dense(units= 6, kernel_initializer = 'uniform', activation = 'relu'))\nclassifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\nclassifier.fit(x_train_std, y_train, batch_size = 10, epochs = 100)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred_test = classifier.predict(x_test_std)\ny_pred_test=y_pred_test>0.5\n\ny_pred_train = classifier.predict(x_train_std)\ny_pred_train=y_pred_train>0.5\n\nprint(\"Accuracy on Train Set ==> \",metrics.accuracy_score(y_train,y_pred_train))\nprint(\"Accuracy on Test Set ==> \",metrics.accuracy_score(y_test,y_pred_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Among all SVC, Random Forest and XGBoost Classifiers are doing well.\n\nI have made 4 notebook on this dataset to show Statistics and Machine Learning. You can read all of them here ==>\n\n1. [Univariate Statistical Analysis](https://www.kaggle.com/ravichaubey1506/univariate-statistical-analysis-on-diabetes)\n2. [Multivariate Staistical Analysis](https://www.kaggle.com/ravichaubey1506/multivariate-statistical-analysis-on-diabetes)\n3. [Inferencial Statistics](https://www.kaggle.com/ravichaubey1506/inferential-statistics-on-diabetes)\n4. [Predective Modelling on Diabtes]()\n\n## Please upvote my Notebook, if it is useful for you. Thank you for reading."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}